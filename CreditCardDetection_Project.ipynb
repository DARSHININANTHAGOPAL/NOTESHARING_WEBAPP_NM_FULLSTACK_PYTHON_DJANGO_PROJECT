{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DARSHININANTHAGOPAL/NOTESHARING_WEBAPP_NM_FULLSTACK_PYTHON_DJANGO_PROJECT/blob/main/CreditCardDetection_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d741659",
      "metadata": {
        "id": "6d741659"
      },
      "outputs": [],
      "source": [
        "# Data Wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Statistics / Logistic Regression\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy import stats\n",
        "\n",
        "#Cross Validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Plotting\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Trees\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#Scores\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "#Model turning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "RANDOM_STATE=42\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df03e5f9",
      "metadata": {
        "id": "df03e5f9",
        "outputId": "48c1bcbb-d9b7-44c3-874f-bf953bb3ea7b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.260648</td>\n",
              "      <td>-0.469648</td>\n",
              "      <td>2.496266</td>\n",
              "      <td>-0.083724</td>\n",
              "      <td>0.129681</td>\n",
              "      <td>0.732898</td>\n",
              "      <td>0.519014</td>\n",
              "      <td>-0.130006</td>\n",
              "      <td>0.727159</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110552</td>\n",
              "      <td>0.217606</td>\n",
              "      <td>-0.134794</td>\n",
              "      <td>0.165959</td>\n",
              "      <td>0.126280</td>\n",
              "      <td>-0.434824</td>\n",
              "      <td>-0.081230</td>\n",
              "      <td>-0.151045</td>\n",
              "      <td>17982.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.985100</td>\n",
              "      <td>-0.356045</td>\n",
              "      <td>0.558056</td>\n",
              "      <td>-0.429654</td>\n",
              "      <td>0.277140</td>\n",
              "      <td>0.428605</td>\n",
              "      <td>0.406466</td>\n",
              "      <td>-0.133118</td>\n",
              "      <td>0.347452</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.194936</td>\n",
              "      <td>-0.605761</td>\n",
              "      <td>0.079469</td>\n",
              "      <td>-0.577395</td>\n",
              "      <td>0.190090</td>\n",
              "      <td>0.296503</td>\n",
              "      <td>-0.248052</td>\n",
              "      <td>-0.064512</td>\n",
              "      <td>6531.37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.260272</td>\n",
              "      <td>-0.949385</td>\n",
              "      <td>1.728538</td>\n",
              "      <td>-0.457986</td>\n",
              "      <td>0.074062</td>\n",
              "      <td>1.419481</td>\n",
              "      <td>0.743511</td>\n",
              "      <td>-0.095576</td>\n",
              "      <td>-0.261297</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005020</td>\n",
              "      <td>0.702906</td>\n",
              "      <td>0.945045</td>\n",
              "      <td>-1.154666</td>\n",
              "      <td>-0.605564</td>\n",
              "      <td>-0.312895</td>\n",
              "      <td>-0.300258</td>\n",
              "      <td>-0.244718</td>\n",
              "      <td>2513.54</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.152152</td>\n",
              "      <td>-0.508959</td>\n",
              "      <td>1.746840</td>\n",
              "      <td>-1.090178</td>\n",
              "      <td>0.249486</td>\n",
              "      <td>1.143312</td>\n",
              "      <td>0.518269</td>\n",
              "      <td>-0.065130</td>\n",
              "      <td>-0.205698</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.146927</td>\n",
              "      <td>-0.038212</td>\n",
              "      <td>-0.214048</td>\n",
              "      <td>-1.893131</td>\n",
              "      <td>1.003963</td>\n",
              "      <td>-0.515950</td>\n",
              "      <td>-0.165316</td>\n",
              "      <td>0.048424</td>\n",
              "      <td>5384.44</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.206820</td>\n",
              "      <td>-0.165280</td>\n",
              "      <td>1.527053</td>\n",
              "      <td>-0.448293</td>\n",
              "      <td>0.106125</td>\n",
              "      <td>0.530549</td>\n",
              "      <td>0.658849</td>\n",
              "      <td>-0.212660</td>\n",
              "      <td>1.049921</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106984</td>\n",
              "      <td>0.729727</td>\n",
              "      <td>-0.161666</td>\n",
              "      <td>0.312561</td>\n",
              "      <td>-0.414116</td>\n",
              "      <td>1.071126</td>\n",
              "      <td>0.023712</td>\n",
              "      <td>0.419117</td>\n",
              "      <td>14278.97</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n",
              "1   1  0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n",
              "2   2 -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n",
              "3   3 -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n",
              "4   4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0 -0.130006  0.727159  ... -0.110552  0.217606 -0.134794  0.165959  0.126280   \n",
              "1 -0.133118  0.347452  ... -0.194936 -0.605761  0.079469 -0.577395  0.190090   \n",
              "2 -0.095576 -0.261297  ... -0.005020  0.702906  0.945045 -1.154666 -0.605564   \n",
              "3 -0.065130 -0.205698  ... -0.146927 -0.038212 -0.214048 -1.893131  1.003963   \n",
              "4 -0.212660  1.049921  ... -0.106984  0.729727 -0.161666  0.312561 -0.414116   \n",
              "\n",
              "        V26       V27       V28    Amount  Class  \n",
              "0 -0.434824 -0.081230 -0.151045  17982.10      0  \n",
              "1  0.296503 -0.248052 -0.064512   6531.37      0  \n",
              "2 -0.312895 -0.300258 -0.244718   2513.54      0  \n",
              "3 -0.515950 -0.165316  0.048424   5384.44      0  \n",
              "4  1.071126  0.023712  0.419117  14278.97      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "credit_original_data = pd.read_csv(\"creditcard_2023.csv\")\n",
        "credit_original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c2805a",
      "metadata": {
        "id": "96c2805a",
        "outputId": "b4ece469-6259-49cb-d728-580a4856a5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568630 entries, 0 to 568629\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   id      568630 non-null  int64  \n",
            " 1   V1      568630 non-null  float64\n",
            " 2   V2      568630 non-null  float64\n",
            " 3   V3      568630 non-null  float64\n",
            " 4   V4      568630 non-null  float64\n",
            " 5   V5      568630 non-null  float64\n",
            " 6   V6      568630 non-null  float64\n",
            " 7   V7      568630 non-null  float64\n",
            " 8   V8      568630 non-null  float64\n",
            " 9   V9      568630 non-null  float64\n",
            " 10  V10     568630 non-null  float64\n",
            " 11  V11     568630 non-null  float64\n",
            " 12  V12     568630 non-null  float64\n",
            " 13  V13     568630 non-null  float64\n",
            " 14  V14     568630 non-null  float64\n",
            " 15  V15     568630 non-null  float64\n",
            " 16  V16     568630 non-null  float64\n",
            " 17  V17     568630 non-null  float64\n",
            " 18  V18     568630 non-null  float64\n",
            " 19  V19     568630 non-null  float64\n",
            " 20  V20     568630 non-null  float64\n",
            " 21  V21     568630 non-null  float64\n",
            " 22  V22     568630 non-null  float64\n",
            " 23  V23     568630 non-null  float64\n",
            " 24  V24     568630 non-null  float64\n",
            " 25  V25     568630 non-null  float64\n",
            " 26  V26     568630 non-null  float64\n",
            " 27  V27     568630 non-null  float64\n",
            " 28  V28     568630 non-null  float64\n",
            " 29  Amount  568630 non-null  float64\n",
            " 30  Class   568630 non-null  int64  \n",
            "dtypes: float64(29), int64(2)\n",
            "memory usage: 134.5 MB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>568630.000000</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>...</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>5.686300e+05</td>\n",
              "      <td>568630.000000</td>\n",
              "      <td>568630.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>284314.500000</td>\n",
              "      <td>-1.109271e-14</td>\n",
              "      <td>-3.429498e-14</td>\n",
              "      <td>-1.209242e-14</td>\n",
              "      <td>3.825991e-15</td>\n",
              "      <td>6.288281e-15</td>\n",
              "      <td>-2.751174e-14</td>\n",
              "      <td>1.240002e-14</td>\n",
              "      <td>8.208047e-15</td>\n",
              "      <td>-1.002980e-14</td>\n",
              "      <td>...</td>\n",
              "      <td>2.210679e-15</td>\n",
              "      <td>-8.767441e-16</td>\n",
              "      <td>4.376179e-16</td>\n",
              "      <td>6.825608e-16</td>\n",
              "      <td>2.545689e-15</td>\n",
              "      <td>1.781906e-15</td>\n",
              "      <td>2.817586e-15</td>\n",
              "      <td>2.891419e-15</td>\n",
              "      <td>12041.957635</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>164149.486121</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>6919.644449</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.495584e+00</td>\n",
              "      <td>-4.996657e+01</td>\n",
              "      <td>-3.183760e+00</td>\n",
              "      <td>-4.951222e+00</td>\n",
              "      <td>-9.952786e+00</td>\n",
              "      <td>-2.111111e+01</td>\n",
              "      <td>-4.351839e+00</td>\n",
              "      <td>-1.075634e+01</td>\n",
              "      <td>-3.751919e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.938252e+01</td>\n",
              "      <td>-7.734798e+00</td>\n",
              "      <td>-3.029545e+01</td>\n",
              "      <td>-4.067968e+00</td>\n",
              "      <td>-1.361263e+01</td>\n",
              "      <td>-8.226969e+00</td>\n",
              "      <td>-1.049863e+01</td>\n",
              "      <td>-3.903524e+01</td>\n",
              "      <td>50.010000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>142157.250000</td>\n",
              "      <td>-5.652859e-01</td>\n",
              "      <td>-4.866777e-01</td>\n",
              "      <td>-6.492987e-01</td>\n",
              "      <td>-6.560203e-01</td>\n",
              "      <td>-2.934955e-01</td>\n",
              "      <td>-4.458712e-01</td>\n",
              "      <td>-2.835329e-01</td>\n",
              "      <td>-1.922572e-01</td>\n",
              "      <td>-5.687446e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.664408e-01</td>\n",
              "      <td>-4.904892e-01</td>\n",
              "      <td>-2.376289e-01</td>\n",
              "      <td>-6.515801e-01</td>\n",
              "      <td>-5.541485e-01</td>\n",
              "      <td>-6.318948e-01</td>\n",
              "      <td>-3.049607e-01</td>\n",
              "      <td>-2.318783e-01</td>\n",
              "      <td>6054.892500</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>284314.500000</td>\n",
              "      <td>-9.363846e-02</td>\n",
              "      <td>-1.358939e-01</td>\n",
              "      <td>3.528579e-04</td>\n",
              "      <td>-7.376152e-02</td>\n",
              "      <td>8.108788e-02</td>\n",
              "      <td>7.871758e-02</td>\n",
              "      <td>2.333659e-01</td>\n",
              "      <td>-1.145242e-01</td>\n",
              "      <td>9.252647e-02</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.743065e-02</td>\n",
              "      <td>-2.732881e-02</td>\n",
              "      <td>-5.968903e-02</td>\n",
              "      <td>1.590123e-02</td>\n",
              "      <td>-8.193162e-03</td>\n",
              "      <td>-1.189208e-02</td>\n",
              "      <td>-1.729111e-01</td>\n",
              "      <td>-1.392973e-02</td>\n",
              "      <td>12030.150000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>426471.750000</td>\n",
              "      <td>8.326582e-01</td>\n",
              "      <td>3.435552e-01</td>\n",
              "      <td>6.285380e-01</td>\n",
              "      <td>7.070047e-01</td>\n",
              "      <td>4.397368e-01</td>\n",
              "      <td>4.977881e-01</td>\n",
              "      <td>5.259548e-01</td>\n",
              "      <td>4.729905e-02</td>\n",
              "      <td>5.592621e-01</td>\n",
              "      <td>...</td>\n",
              "      <td>1.479787e-01</td>\n",
              "      <td>4.638817e-01</td>\n",
              "      <td>1.557153e-01</td>\n",
              "      <td>7.007374e-01</td>\n",
              "      <td>5.500147e-01</td>\n",
              "      <td>6.728879e-01</td>\n",
              "      <td>3.340230e-01</td>\n",
              "      <td>4.095903e-01</td>\n",
              "      <td>18036.330000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>568629.000000</td>\n",
              "      <td>2.229046e+00</td>\n",
              "      <td>4.361865e+00</td>\n",
              "      <td>1.412583e+01</td>\n",
              "      <td>3.201536e+00</td>\n",
              "      <td>4.271689e+01</td>\n",
              "      <td>2.616840e+01</td>\n",
              "      <td>2.178730e+02</td>\n",
              "      <td>5.958040e+00</td>\n",
              "      <td>2.027006e+01</td>\n",
              "      <td>...</td>\n",
              "      <td>8.087080e+00</td>\n",
              "      <td>1.263251e+01</td>\n",
              "      <td>3.170763e+01</td>\n",
              "      <td>1.296564e+01</td>\n",
              "      <td>1.462151e+01</td>\n",
              "      <td>5.623285e+00</td>\n",
              "      <td>1.132311e+02</td>\n",
              "      <td>7.725594e+01</td>\n",
              "      <td>24039.930000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id            V1            V2            V3            V4  \\\n",
              "count  568630.000000  5.686300e+05  5.686300e+05  5.686300e+05  5.686300e+05   \n",
              "mean   284314.500000 -1.109271e-14 -3.429498e-14 -1.209242e-14  3.825991e-15   \n",
              "std    164149.486121  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min         0.000000 -3.495584e+00 -4.996657e+01 -3.183760e+00 -4.951222e+00   \n",
              "25%    142157.250000 -5.652859e-01 -4.866777e-01 -6.492987e-01 -6.560203e-01   \n",
              "50%    284314.500000 -9.363846e-02 -1.358939e-01  3.528579e-04 -7.376152e-02   \n",
              "75%    426471.750000  8.326582e-01  3.435552e-01  6.285380e-01  7.070047e-01   \n",
              "max    568629.000000  2.229046e+00  4.361865e+00  1.412583e+01  3.201536e+00   \n",
              "\n",
              "                 V5            V6            V7            V8            V9  \\\n",
              "count  5.686300e+05  5.686300e+05  5.686300e+05  5.686300e+05  5.686300e+05   \n",
              "mean   6.288281e-15 -2.751174e-14  1.240002e-14  8.208047e-15 -1.002980e-14   \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min   -9.952786e+00 -2.111111e+01 -4.351839e+00 -1.075634e+01 -3.751919e+00   \n",
              "25%   -2.934955e-01 -4.458712e-01 -2.835329e-01 -1.922572e-01 -5.687446e-01   \n",
              "50%    8.108788e-02  7.871758e-02  2.333659e-01 -1.145242e-01  9.252647e-02   \n",
              "75%    4.397368e-01  4.977881e-01  5.259548e-01  4.729905e-02  5.592621e-01   \n",
              "max    4.271689e+01  2.616840e+01  2.178730e+02  5.958040e+00  2.027006e+01   \n",
              "\n",
              "       ...           V21           V22           V23           V24  \\\n",
              "count  ...  5.686300e+05  5.686300e+05  5.686300e+05  5.686300e+05   \n",
              "mean   ...  2.210679e-15 -8.767441e-16  4.376179e-16  6.825608e-16   \n",
              "std    ...  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min    ... -1.938252e+01 -7.734798e+00 -3.029545e+01 -4.067968e+00   \n",
              "25%    ... -1.664408e-01 -4.904892e-01 -2.376289e-01 -6.515801e-01   \n",
              "50%    ... -3.743065e-02 -2.732881e-02 -5.968903e-02  1.590123e-02   \n",
              "75%    ...  1.479787e-01  4.638817e-01  1.557153e-01  7.007374e-01   \n",
              "max    ...  8.087080e+00  1.263251e+01  3.170763e+01  1.296564e+01   \n",
              "\n",
              "                V25           V26           V27           V28         Amount  \\\n",
              "count  5.686300e+05  5.686300e+05  5.686300e+05  5.686300e+05  568630.000000   \n",
              "mean   2.545689e-15  1.781906e-15  2.817586e-15  2.891419e-15   12041.957635   \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00    6919.644449   \n",
              "min   -1.361263e+01 -8.226969e+00 -1.049863e+01 -3.903524e+01      50.010000   \n",
              "25%   -5.541485e-01 -6.318948e-01 -3.049607e-01 -2.318783e-01    6054.892500   \n",
              "50%   -8.193162e-03 -1.189208e-02 -1.729111e-01 -1.392973e-02   12030.150000   \n",
              "75%    5.500147e-01  6.728879e-01  3.340230e-01  4.095903e-01   18036.330000   \n",
              "max    1.462151e+01  5.623285e+00  1.132311e+02  7.725594e+01   24039.930000   \n",
              "\n",
              "          Class  \n",
              "count  568630.0  \n",
              "mean        0.5  \n",
              "std         0.5  \n",
              "min         0.0  \n",
              "25%         0.0  \n",
              "50%         0.5  \n",
              "75%         1.0  \n",
              "max         1.0  \n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cc_data = credit_original_data.copy()\n",
        "cc_data.info()\n",
        "cc_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c58a9e96",
      "metadata": {
        "id": "c58a9e96",
        "outputId": "57a77a2b-9e68-4327-f09a-7a7df9663cac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Generalized Linear Model Regression Results                  \n",
            "==============================================================================\n",
            "Dep. Variable:                  Class   No. Observations:               568630\n",
            "Model:                            GLM   Df Residuals:                   568600\n",
            "Model Family:                Binomial   Df Model:                           29\n",
            "Link Function:                  logit   Scale:                          1.0000\n",
            "Method:                          IRLS   Log-Likelihood:                -53585.\n",
            "Date:                Sat, 04 Nov 2023   Deviance:                   1.0717e+05\n",
            "Time:                        18:38:15   Pearson chi2:                 7.82e+16\n",
            "No. Iterations:                    13                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      9.0810      0.084    108.499      0.000       8.917       9.245\n",
            "V1            -0.6880      0.019    -36.668      0.000      -0.725      -0.651\n",
            "V2             0.1582      0.016      9.703      0.000       0.126       0.190\n",
            "V3            -1.1804      0.020    -59.558      0.000      -1.219      -1.142\n",
            "V4             3.6329      0.027    136.191      0.000       3.581       3.685\n",
            "V5            -0.0153      0.014     -1.057      0.290      -0.044       0.013\n",
            "V6            -0.4789      0.017    -28.909      0.000      -0.511      -0.446\n",
            "V7            -1.1092      0.026    -42.289      0.000      -1.161      -1.058\n",
            "V8            -2.8335      0.045    -63.570      0.000      -2.921      -2.746\n",
            "V9            -0.4779      0.022    -21.598      0.000      -0.521      -0.435\n",
            "V10           -1.8979      0.031    -60.689      0.000      -1.959      -1.837\n",
            "V11            1.8757      0.018    103.216      0.000       1.840       1.911\n",
            "V12           -2.8171      0.026   -109.223      0.000      -2.868      -2.767\n",
            "V13            0.0176      0.009      2.014      0.044       0.000       0.035\n",
            "V14           -3.3569      0.026   -127.275      0.000      -3.409      -3.305\n",
            "V15           -0.2418      0.009    -27.972      0.000      -0.259      -0.225\n",
            "V16           -0.8542      0.024    -35.459      0.000      -0.901      -0.807\n",
            "V17           -1.9342      0.027    -71.811      0.000      -1.987      -1.881\n",
            "V18           -0.9296      0.021    -45.195      0.000      -0.970      -0.889\n",
            "V19           -0.0804      0.012     -6.443      0.000      -0.105      -0.056\n",
            "V20            0.1328      0.012     10.902      0.000       0.109       0.157\n",
            "V21            0.2487      0.028      8.737      0.000       0.193       0.304\n",
            "V22            0.4401      0.015     29.085      0.000       0.410       0.470\n",
            "V23           -0.3412      0.011    -31.866      0.000      -0.362      -0.320\n",
            "V24           -0.1660      0.009    -18.049      0.000      -0.184      -0.148\n",
            "V25            0.1617      0.011     14.900      0.000       0.140       0.183\n",
            "V26           -0.1069      0.010    -10.920      0.000      -0.126      -0.088\n",
            "V27            0.1873      0.022      8.642      0.000       0.145       0.230\n",
            "V28            0.1534      0.011     14.578      0.000       0.133       0.174\n",
            "Amount     -7.252e-08   1.17e-06     -0.062      0.950   -2.36e-06    2.22e-06\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "#perform logistic regression using glm (generalized linear model) method\n",
        "logit_equation = 'Class~V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17+V18+V19+V20+V21+V22+V23+V24+V25+V26+V27+V28+Amount'\n",
        "fit1 = smf.glm(logit_equation, data=cc_data, family=sm.families.Binomial()).fit()\n",
        "print(fit1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd46b85",
      "metadata": {
        "id": "1dd46b85",
        "outputId": "6f89b480-72b8-4451-d8af-7398720c8e02"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.260648</td>\n",
              "      <td>-0.469648</td>\n",
              "      <td>2.496266</td>\n",
              "      <td>-0.083724</td>\n",
              "      <td>0.129681</td>\n",
              "      <td>17982.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.985100</td>\n",
              "      <td>-0.356045</td>\n",
              "      <td>0.558056</td>\n",
              "      <td>-0.429654</td>\n",
              "      <td>0.277140</td>\n",
              "      <td>6531.37</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-0.260272</td>\n",
              "      <td>-0.949385</td>\n",
              "      <td>1.728538</td>\n",
              "      <td>-0.457986</td>\n",
              "      <td>0.074062</td>\n",
              "      <td>2513.54</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.152152</td>\n",
              "      <td>-0.508959</td>\n",
              "      <td>1.746840</td>\n",
              "      <td>-1.090178</td>\n",
              "      <td>0.249486</td>\n",
              "      <td>5384.44</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-0.206820</td>\n",
              "      <td>-0.165280</td>\n",
              "      <td>1.527053</td>\n",
              "      <td>-0.448293</td>\n",
              "      <td>0.106125</td>\n",
              "      <td>14278.97</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id        V1        V2        V3        V4        V5    Amount  Class\n",
              "0   0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  17982.10      0\n",
              "1   1  0.985100 -0.356045  0.558056 -0.429654  0.277140   6531.37      0\n",
              "2   2 -0.260272 -0.949385  1.728538 -0.457986  0.074062   2513.54      0\n",
              "3   3 -0.152152 -0.508959  1.746840 -1.090178  0.249486   5384.44      0\n",
              "4   4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  14278.97      0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Columns to keep\n",
        "target_columns = ['id', 'V1', 'V2', 'V3', 'V4', 'V5', 'Amount', 'Class']\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "cc_data = cc_data[target_columns]\n",
        "cc_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff05d46b",
      "metadata": {
        "id": "ff05d46b",
        "outputId": "06b514ca-b68e-4d24-859b-8ae46069a7a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-098ceec9e8bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create subplots for the histograms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# Define the columns you want to plot\n",
        "columns_to_plot = ['V1', 'V2', 'V3', 'V4', 'V5', 'Amount']\n",
        "\n",
        "# Create subplots for the histograms\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "for i, col in enumerate(columns_to_plot):\n",
        "    row_idx = i // 3\n",
        "    col_idx = i % 3\n",
        "\n",
        "    sns.histplot(data=cc_data, x=col, kde=True, color='blue', ax=axes[row_idx, col_idx])\n",
        "    axes[row_idx, col_idx].set_title(f'Histogram of {col}')\n",
        "    axes[row_idx, col_idx].set_xlabel(col)\n",
        "    axes[row_idx, col_idx].set_ylabel('Frequency')\n",
        "\n",
        "# Remove empty subplots, if any\n",
        "for i in range(len(columns_to_plot), 2 * 3):\n",
        "    fig.delaxes(axes.flatten()[i])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447908c5",
      "metadata": {
        "id": "447908c5",
        "outputId": "d9309bed-6907-4997-c6f6-b976f9565009"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count of Fraudulent Transactions: 284315\n",
            "Count of Non-Fraudulent Transactions: 284315\n"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Class', data=cc_data, palette='Set1')\n",
        "plt.title('Distribution of Classes')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "fraud_count = (cc_data['Class']==1).sum()\n",
        "nonfraud_count = (cc_data['Class']==0).sum()\n",
        "print(\"Count of Fraudulent Transactions:\", fraud_count)\n",
        "print(\"Count of Non-Fraudulent Transactions:\", nonfraud_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9e6ee0",
      "metadata": {
        "scrolled": true,
        "id": "eb9e6ee0",
        "outputId": "48b23a36-a730-458a-b4e2-7dc6e0c981b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Generalized Linear Model Regression Results                  \n",
            "==============================================================================\n",
            "Dep. Variable:                  Class   No. Observations:               568630\n",
            "Model:                            GLM   Df Residuals:                   568624\n",
            "Model Family:                Binomial   Df Model:                            5\n",
            "Link Function:                  logit   Scale:                          1.0000\n",
            "Method:                          IRLS   Log-Likelihood:            -1.0149e+05\n",
            "Date:                Sat, 04 Nov 2023   Deviance:                   2.0298e+05\n",
            "Time:                        18:38:31   Pearson chi2:                 3.39e+08\n",
            "No. Iterations:                     9                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "Intercept      1.3717      0.009    158.461      0.000       1.355       1.389\n",
            "V1            -1.1214      0.008   -148.395      0.000      -1.136      -1.107\n",
            "V2             0.8378      0.009     97.750      0.000       0.821       0.855\n",
            "V3            -2.2584      0.011   -209.723      0.000      -2.280      -2.237\n",
            "V4             3.4206      0.013    260.838      0.000       3.395       3.446\n",
            "V5            -0.1120      0.008    -13.584      0.000      -0.128      -0.096\n",
            "==============================================================================\n"
          ]
        }
      ],
      "source": [
        "#perform logistic regression using glm (generalized linear model) method.\n",
        "#Note, we leave out Amount as it isn't statistically significant\n",
        "logit_eq = 'Class~V1+V2+V3+V4+V5'\n",
        "fit1 = smf.glm(logit_eq, data=cc_data, family=sm.families.Binomial()).fit()\n",
        "print(fit1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a428e7",
      "metadata": {
        "id": "90a428e7"
      },
      "outputs": [],
      "source": [
        "#split data into training and validation/test set\n",
        "train, test = train_test_split(cc_data, test_size=0.2, random_state = 42)\n",
        "\n",
        "#Fit logistic regression model with training set\n",
        "fit2 = smf.glm(logit_eq, data=train, family=sm.families.Binomial()).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fde4f3f",
      "metadata": {
        "id": "9fde4f3f",
        "outputId": "14be306b-7a99-4772-f1c0-bae865da9ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our model has a misclassifcation rate of: 6.686245889242566 %\n"
          ]
        }
      ],
      "source": [
        "#Make predictions on the validation set with the new model\n",
        "predictions = fit2.predict(test)\n",
        "\n",
        "#Convert predicted probabilities to binary predictions: 1 if prob>0.5\n",
        "encode = lambda x: 1 if x>=0.5 else 0\n",
        "predicted = predictions.map(encode)\n",
        "\n",
        "# compare the predicted values with the actual values in the test set\n",
        "misclassified = (predicted != test['Class']).sum()\n",
        "\n",
        "mis_rate = misclassified/len(test)\n",
        "print(\"Our model has a misclassifcation rate of:\", mis_rate*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7922be24",
      "metadata": {
        "id": "7922be24",
        "outputId": "f25ebc62-ed7c-421a-ada3-c0619b29d49c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "true_labels = test['Class']\n",
        "cm = confusion_matrix(true_labels, predicted)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Fraudulent', 'Fraudulent'], yticklabels=['Non-Fraudulent', 'Fraudulent'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3c5b4fd",
      "metadata": {
        "id": "f3c5b4fd"
      },
      "outputs": [],
      "source": [
        "test_sizes = [0.1, 0.3, 0.5]\n",
        "#to store the misclassification accuracy score for each model\n",
        "model_rates = []\n",
        "#to store the model parameters\n",
        "model_params = []\n",
        "\n",
        "#iterate through each test size\n",
        "for test_size in test_sizes:\n",
        "\n",
        "    #split data into training and validation sets\n",
        "    train, test = train_test_split(cc_data, test_size=test_size, random_state = 42)\n",
        "\n",
        "    #fit the logistic regression model on training set\n",
        "    model = smf.glm(logit_eq, data=train, family=sm.families.Binomial()).fit()\n",
        "\n",
        "    #Store model parameters\n",
        "    model_params.append(model.params)\n",
        "\n",
        "    #Get predictions on test/validation test\n",
        "    predicted_probs = model.predict(test)\n",
        "\n",
        "    #Classifdy predictions on 0.5 threshold\n",
        "    predicted = predicted_probs.map(encode)\n",
        "\n",
        "    #Calculate misclassification rate & store rates\n",
        "    misclassified = (predicted != test['Class']).sum()\n",
        "    mis_rate = misclassified / len(test)\n",
        "    model_rates.append(mis_rate)\n",
        "\n",
        "#Store misclassification rates in designated variables\n",
        "mis_rate_10, mis_rate_30, mis_rate_50 = model_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a532d9",
      "metadata": {
        "id": "75a532d9",
        "outputId": "d151538b-b62b-4b38-8811-a8cd16ab2e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Misclassification Rate for 10% Test Size: 6.853314105833319 %\n",
            "Misclassification Rate for 30% Test Size: 6.631728892249793 %\n",
            "Misclassification Rate for 50% Test Size: 6.641225401403374 %\n"
          ]
        }
      ],
      "source": [
        "print(f\"Misclassification Rate for 10% Test Size:\", mis_rate_10*100, \"%\")\n",
        "print(f\"Misclassification Rate for 30% Test Size:\", mis_rate_30*100, \"%\")\n",
        "print(f\"Misclassification Rate for 50% Test Size:\", mis_rate_50*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b070852",
      "metadata": {
        "id": "6b070852",
        "outputId": "a6718d8f-382e-40a5-dbee-4a862778316c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.06621212387668608\n"
          ]
        }
      ],
      "source": [
        "#Store all miclassification rates\n",
        "misclass_rates = []\n",
        "\n",
        "model = LogisticRegression() #Define ML Model\n",
        "\n",
        "for trial in range(10):\n",
        "    #Define cross-value method\n",
        "    cv_method = KFold(n_splits = 10, shuffle=True, random_state = trial)\n",
        "\n",
        "    #perform cv and get accuracy scores for each fold\n",
        "    scores = cross_val_score(model, cc_data[['V1','V2','V3','V4','V5']], cc_data['Class'], cv=cv_method, scoring ='accuracy')\n",
        "\n",
        "    #calculate misclassification rate for each fold\n",
        "    misclass_rate = 1 - scores\n",
        "    misclass_rates.extend(misclass_rate)\n",
        "\n",
        "mis_rate_kfold =sum(misclass_rates) / len(misclass_rates)\n",
        "print(mis_rate_kfold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bab9a91",
      "metadata": {
        "id": "8bab9a91",
        "outputId": "f731aad4-3be4-46df-acf7-2744f482ab70"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.260648</td>\n",
              "      <td>-0.469648</td>\n",
              "      <td>2.496266</td>\n",
              "      <td>-0.083724</td>\n",
              "      <td>0.129681</td>\n",
              "      <td>0.732898</td>\n",
              "      <td>0.519014</td>\n",
              "      <td>-0.130006</td>\n",
              "      <td>0.727159</td>\n",
              "      <td>0.637735</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091202</td>\n",
              "      <td>-0.110552</td>\n",
              "      <td>0.217606</td>\n",
              "      <td>-0.134794</td>\n",
              "      <td>0.165959</td>\n",
              "      <td>0.126280</td>\n",
              "      <td>-0.434824</td>\n",
              "      <td>-0.081230</td>\n",
              "      <td>-0.151045</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.985100</td>\n",
              "      <td>-0.356045</td>\n",
              "      <td>0.558056</td>\n",
              "      <td>-0.429654</td>\n",
              "      <td>0.277140</td>\n",
              "      <td>0.428605</td>\n",
              "      <td>0.406466</td>\n",
              "      <td>-0.133118</td>\n",
              "      <td>0.347452</td>\n",
              "      <td>0.529808</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.233984</td>\n",
              "      <td>-0.194936</td>\n",
              "      <td>-0.605761</td>\n",
              "      <td>0.079469</td>\n",
              "      <td>-0.577395</td>\n",
              "      <td>0.190090</td>\n",
              "      <td>0.296503</td>\n",
              "      <td>-0.248052</td>\n",
              "      <td>-0.064512</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.260272</td>\n",
              "      <td>-0.949385</td>\n",
              "      <td>1.728538</td>\n",
              "      <td>-0.457986</td>\n",
              "      <td>0.074062</td>\n",
              "      <td>1.419481</td>\n",
              "      <td>0.743511</td>\n",
              "      <td>-0.095576</td>\n",
              "      <td>-0.261297</td>\n",
              "      <td>0.690708</td>\n",
              "      <td>...</td>\n",
              "      <td>0.361652</td>\n",
              "      <td>-0.005020</td>\n",
              "      <td>0.702906</td>\n",
              "      <td>0.945045</td>\n",
              "      <td>-1.154666</td>\n",
              "      <td>-0.605564</td>\n",
              "      <td>-0.312895</td>\n",
              "      <td>-0.300258</td>\n",
              "      <td>-0.244718</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.152152</td>\n",
              "      <td>-0.508959</td>\n",
              "      <td>1.746840</td>\n",
              "      <td>-1.090178</td>\n",
              "      <td>0.249486</td>\n",
              "      <td>1.143312</td>\n",
              "      <td>0.518269</td>\n",
              "      <td>-0.065130</td>\n",
              "      <td>-0.205698</td>\n",
              "      <td>0.575231</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.378223</td>\n",
              "      <td>-0.146927</td>\n",
              "      <td>-0.038212</td>\n",
              "      <td>-0.214048</td>\n",
              "      <td>-1.893131</td>\n",
              "      <td>1.003963</td>\n",
              "      <td>-0.515950</td>\n",
              "      <td>-0.165316</td>\n",
              "      <td>0.048424</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.206820</td>\n",
              "      <td>-0.165280</td>\n",
              "      <td>1.527053</td>\n",
              "      <td>-0.448293</td>\n",
              "      <td>0.106125</td>\n",
              "      <td>0.530549</td>\n",
              "      <td>0.658849</td>\n",
              "      <td>-0.212660</td>\n",
              "      <td>1.049921</td>\n",
              "      <td>0.968046</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247237</td>\n",
              "      <td>-0.106984</td>\n",
              "      <td>0.729727</td>\n",
              "      <td>-0.161666</td>\n",
              "      <td>0.312561</td>\n",
              "      <td>-0.414116</td>\n",
              "      <td>1.071126</td>\n",
              "      <td>0.023712</td>\n",
              "      <td>0.419117</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n",
              "1  0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n",
              "2 -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n",
              "3 -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n",
              "4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n",
              "\n",
              "         V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
              "0 -0.130006  0.727159  0.637735  ...  0.091202 -0.110552  0.217606 -0.134794   \n",
              "1 -0.133118  0.347452  0.529808  ... -0.233984 -0.194936 -0.605761  0.079469   \n",
              "2 -0.095576 -0.261297  0.690708  ...  0.361652 -0.005020  0.702906  0.945045   \n",
              "3 -0.065130 -0.205698  0.575231  ... -0.378223 -0.146927 -0.038212 -0.214048   \n",
              "4 -0.212660  1.049921  0.968046  ...  0.247237 -0.106984  0.729727 -0.161666   \n",
              "\n",
              "        V24       V25       V26       V27       V28  Class  \n",
              "0  0.165959  0.126280 -0.434824 -0.081230 -0.151045      0  \n",
              "1 -0.577395  0.190090  0.296503 -0.248052 -0.064512      0  \n",
              "2 -1.154666 -0.605564 -0.312895 -0.300258 -0.244718      0  \n",
              "3 -1.893131  1.003963 -0.515950 -0.165316  0.048424      0  \n",
              "4  0.312561 -0.414116  1.071126  0.023712  0.419117      0  \n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Get get original dataset\n",
        "cc_data = credit_original_data.copy()\n",
        "cc_data = cc_data.drop(columns = ['id', 'Amount'])\n",
        "\n",
        "cc_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7379af6",
      "metadata": {
        "id": "f7379af6",
        "outputId": "1121b3bd-6233-4a61-cd27-310b2c9d31e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The feature that has the highest correlation with Credit Card Fraud is: V4\n"
          ]
        }
      ],
      "source": [
        "# Calculate pairwise correlations\n",
        "corr_matrix = cc_data.corr()\n",
        "# Find the correlation of 'HD' with all predictor variables\n",
        "corr_HD = corr_matrix['Class']\n",
        "\n",
        "# Find the predictor with the highest positive correlation\n",
        "highest_corr = corr_HD.drop('Class').idxmax()\n",
        "\n",
        "print(f\"The feature that has the highest correlation with Credit Card Fraud is: {highest_corr}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fee360f",
      "metadata": {
        "id": "4fee360f"
      },
      "outputs": [],
      "source": [
        "X = cc_data.drop('Class', axis=1) # All predictor variables\n",
        "y = cc_data['Class'] # Target Output\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee15bcd",
      "metadata": {
        "id": "2ee15bcd",
        "outputId": "ba214ee2-c905-4753-e061-087630824a79"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "BaggingClassifier(random_state=42)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create Bagging Classifier\n",
        "bag = BaggingClassifier(random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bag.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0205bef",
      "metadata": {
        "id": "c0205bef",
        "outputId": "65f2eb0d-a63e-4794-a6ad-d3aa9ff3aa6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bagging Classifier - Precision: 0.9991053730243654\n",
            "Bagging Classifier - Recall: 0.9996489750070205\n",
            "Bagging Classifier - Accuracy: 0.9993756924537924\n"
          ]
        }
      ],
      "source": [
        "# Make Predictions using BagClassifier on test data\n",
        "y_pred = bag.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and Accuracy Scores\n",
        "bag_precision = precision_score(y_test, y_pred)\n",
        "bag_recall = recall_score(y_test, y_pred)\n",
        "bag_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Bagging Classifier - Precision: {bag_precision}\")\n",
        "print(f\"Bagging Classifier - Recall: {bag_recall}\")\n",
        "print(f\"Bagging Classifier - Accuracy: {bag_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd6de16",
      "metadata": {
        "id": "ebd6de16",
        "outputId": "c3c9ae2e-0ca1-40a7-b24d-e47fde13af1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bagging misclassification rate: 0.06%\n"
          ]
        }
      ],
      "source": [
        "bag_misclass_rate = (1 - bag_accuracy)*100\n",
        "print(f\"Bagging misclassification rate: {bag_misclass_rate:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264500ba",
      "metadata": {
        "id": "264500ba",
        "outputId": "bb2ec0e9-8a2a-4517-e131-c01412e306e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# Define the hyperparameter grid to search through (range 5 to 29)\\nparams = {\\'n_estimators\\': range(5, 30)}\\n\\n# Create GridSearchCV with 10-Fold CV\\ngrid_search = GridSearchCV(bag, params, cv=10, scoring=\\'accuracy\\')\\n\\n# Fit GridSearchCV to training data\\ngrid_search.fit(X_train, y_train)\\n\\n# Get best estimator with tuned paramters\\nbest_bag = grid_search.best_estimator_\\n\\n# Make predictions on training data\\ny_pred = best_bag.predict(X_test)\\n\\nbag_tuned_precision = precision_score(y_test, y_pred)\\nbag_tuned_recall = recall_score(y_test, y_pred)\\nbag_tuned_accuracy = accuracy_score(y_test, y_pred)\\n\\n# Store best performing # of estimators\\n#number = best_bag.n_estimators\\nbag_best_param = {\\'n_estimators\\': best_bag.n_estimators}\\n\\nprint(f\"Tuned Bagging Classifier - Precision: {bag_tuned_precision}\")\\nprint(f\"Tuned Bagging Classifier - Recall: {bag_tuned_recall}\")\\nprint(f\"Tuned Bagging Classifier - Accuracy: {bag_tuned_accuracy}\")\\nprint(bag_best_param)\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Define the hyperparameter grid to search through (range 5 to 29)\n",
        "params = {'n_estimators': range(5, 30)}\n",
        "\n",
        "# Create GridSearchCV with 10-Fold CV\n",
        "grid_search = GridSearchCV(bag, params, cv=10, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best estimator with tuned paramters\n",
        "best_bag = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on training data\n",
        "y_pred = best_bag.predict(X_test)\n",
        "\n",
        "bag_tuned_precision = precision_score(y_test, y_pred)\n",
        "bag_tuned_recall = recall_score(y_test, y_pred)\n",
        "bag_tuned_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Store best performing # of estimators\n",
        "#number = best_bag.n_estimators\n",
        "bag_best_param = {'n_estimators': best_bag.n_estimators}\n",
        "\n",
        "print(f\"Tuned Bagging Classifier - Precision: {bag_tuned_precision}\")\n",
        "print(f\"Tuned Bagging Classifier - Recall: {bag_tuned_recall}\")\n",
        "print(f\"Tuned Bagging Classifier - Accuracy: {bag_tuned_accuracy}\")\n",
        "print(bag_best_param)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77245f73",
      "metadata": {
        "id": "77245f73",
        "outputId": "d5fbd185-2766-48c1-bda6-364d63850467"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create Random Forest Classifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "#Train rf classifier\n",
        "rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70ffba1",
      "metadata": {
        "id": "c70ffba1",
        "outputId": "f11213aa-eba4-4b20-e231-e96e449832e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest Classifier - Precision: 0.9997543428671697\n",
            "RandomForest Classifier - Recall: 1.0\n",
            "RandomForest Classifier - Accuracy: 0.9998768971035648\n"
          ]
        }
      ],
      "source": [
        "# Predict test outcomes using random forest classifier\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"RandomForest Classifier - Precision: {rf_precision}\")\n",
        "print(f\"RandomForest Classifier - Recall: {rf_recall}\")\n",
        "print(f\"RandomForest Classifier - Accuracy: {rf_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113a59a9",
      "metadata": {
        "id": "113a59a9",
        "outputId": "bbcc2b88-f3a3-4551-836a-02d87f65277c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest misclassification rate: 0.01%\n"
          ]
        }
      ],
      "source": [
        "rf_misclass_rate = (1 - rf_accuracy)*100\n",
        "print(f\"Random Forest misclassification rate: {rf_misclass_rate:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17353873",
      "metadata": {
        "id": "17353873",
        "outputId": "6fb31636-ae44-47f5-d72f-817d4a4037e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature V1: Importance = 0.0120\n",
            "Feature V2: Importance = 0.0326\n",
            "Feature V3: Importance = 0.0612\n",
            "Feature V4: Importance = 0.1463\n",
            "Feature V5: Importance = 0.0070\n",
            "Feature V6: Importance = 0.0068\n",
            "Feature V7: Importance = 0.0173\n",
            "Feature V8: Importance = 0.0108\n",
            "Feature V9: Importance = 0.0186\n",
            "Feature V10: Importance = 0.1262\n",
            "Feature V11: Importance = 0.0955\n",
            "Feature V12: Importance = 0.0714\n",
            "Feature V13: Importance = 0.0064\n",
            "Feature V14: Importance = 0.1842\n",
            "Feature V15: Importance = 0.0058\n",
            "Feature V16: Importance = 0.0538\n",
            "Feature V17: Importance = 0.0741\n",
            "Feature V18: Importance = 0.0068\n",
            "Feature V19: Importance = 0.0075\n",
            "Feature V20: Importance = 0.0057\n",
            "Feature V21: Importance = 0.0152\n",
            "Feature V22: Importance = 0.0041\n",
            "Feature V23: Importance = 0.0056\n",
            "Feature V24: Importance = 0.0043\n",
            "Feature V25: Importance = 0.0056\n",
            "Feature V26: Importance = 0.0057\n",
            "Feature V27: Importance = 0.0043\n",
            "Feature V28: Importance = 0.0055\n"
          ]
        }
      ],
      "source": [
        "# Access the feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Get the feature names from your dataset\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Print all features and their importances\n",
        "for feature_name, importance in zip(feature_names, feature_importances):\n",
        "    print(f\"Feature {feature_name}: Importance = {importance:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6454cc98",
      "metadata": {
        "id": "6454cc98",
        "outputId": "41930766-fada-4000-c102-1f611c32e180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Define our hyperparamters\\nparams = {\\'max_depth\\':range(3,20), # Range of tree depth\\n          \\'n_estimators\\':range(10,29) # Range of # of trees\\n         }\\n\\n# Create GridSearchCV for RandomForest Classifier with 10fold CV\\ngrid_search_rf = GridSearchCV(rf, params, cv=10, scoring=\\'accuracy\\')\\n\\n# Fit GridSearch with training data\\ngrid_search_rf.fit(X_train, y_train)\\n\\n# Get best estimators with tuned hyperparamters \\nbest_rf = grid_search_rf.best_estimator_\\n\\n# Make predictions using tuned rf classifier\\ny_pred = best_rf.predict(X_test)\\n\\nrf_tuned_precision = precision_score(y_test, y_pred)\\nrf_tuned_recall = recall_score(y_test, y_pred)\\nrf_tuned_accuracy = accuracy_score(y_test, y_pred)\\n\\nrf_tuned = {\\'max_depth\\': best_rf.max_depth, \\'n_estimators\\': best_rf.n_estimators}\\n\\nprint(f\"Tuned Random Forest - Accuracy: {rf_tuned_accuracy}\")\\nprint(f\"Tuned Random Forest - Recall: {rf_tuned_recall}\")\\nprint(f\"Tuned Random Forest - Precision: {rf_tuned_precision}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Define our hyperparamters\n",
        "params = {'max_depth':range(3,20), # Range of tree depth\n",
        "          'n_estimators':range(10,29) # Range of # of trees\n",
        "         }\n",
        "\n",
        "# Create GridSearchCV for RandomForest Classifier with 10fold CV\n",
        "grid_search_rf = GridSearchCV(rf, params, cv=10, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearch with training data\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Get best estimators with tuned hyperparamters\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Make predictions using tuned rf classifier\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "rf_tuned_precision = precision_score(y_test, y_pred)\n",
        "rf_tuned_recall = recall_score(y_test, y_pred)\n",
        "rf_tuned_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "rf_tuned = {'max_depth': best_rf.max_depth, 'n_estimators': best_rf.n_estimators}\n",
        "\n",
        "print(f\"Tuned Random Forest - Accuracy: {rf_tuned_accuracy}\")\n",
        "print(f\"Tuned Random Forest - Recall: {rf_tuned_recall}\")\n",
        "print(f\"Tuned Random Forest - Precision: {rf_tuned_precision}\")\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}